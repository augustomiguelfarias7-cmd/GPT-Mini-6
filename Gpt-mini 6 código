import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset, concatenate_datasets
from PIL import Image
import torchvision.transforms as T
import matplotlib.pyplot as plt
import subprocess, requests, os
from bs4 import BeautifulSoup

# Tokenizer
class SimpleTokenizer:
    def __init__(self, vocab_size=2_000_000):
        self.vocab = {f"TOKEN{i}":i for i in range(vocab_size)}
        self.vocab["[PAD]"]=0; self.vocab["[UNK]"]=1; self.vocab_size=len(self.vocab)
    def encode(self,text): return [self.vocab.get(t,self.vocab["[UNK]"]) for t in text.split()]
    def decode(self,ids): inv={v:k for k,v in self.vocab.items()}; return " ".join([inv.get(i,"[UNK]") for i in ids])

# Transformer block
class AdvancedTransformerBlock(nn.Module):
    def __init__(self, d, heads, ff): super().__init__()
        self.attn=nn.MultiheadAttention(d,heads,batch_first=True)
        self.norm1=nn.LayerNorm(d); self.ff=nn.Sequential(nn.Linear(d,ff),nn.GELU(),nn.Linear(ff,d))
        self.norm2=nn.LayerNorm(d)
    def forward(self,x): a,_=self.attn(x,x,x); x=self.norm1(x+a); x=self.norm2(x+self.ff(x)); return x

# VAE
class ImageVAE(nn.Module):
    def __init__(self,dim=4096):
        super().__init__(); d2,d4,d8=dim*2,dim*4,dim*8
        self.enc=nn.Sequential(nn.Conv2d(3,dim,4,2,1),nn.ReLU(),
                               nn.Conv2d(dim,d2,4,2,1),nn.ReLU(),
                               nn.Conv2d(d2,d4,4,2,1),nn.ReLU(),
                               nn.Conv2d(d4,d8,4,2,1),nn.ReLU(),
                               nn.Conv2d(d8,dim,4,2,1))
        self.dec=nn.Sequential(nn.ConvTranspose2d(dim,d8,4,2,1),nn.ReLU(),
                               nn.ConvTranspose2d(d8,d4,4,2,1),nn.ReLU(),
                               nn.ConvTranspose2d(d4,d2,4,2,1),nn.ReLU(),
                               nn.ConvTranspose2d(d2,dim,4,2,1),nn.ReLU(),
                               nn.ConvTranspose2d(dim,3,4,2,1),nn.Sigmoid())
    def encode(self,x): return self.enc(x)
    def decode(self,z): return self.dec(z)
    def show(self,t): plt.imshow(t.detach().cpu().clamp(0,1).permute(1,2,0));plt.axis('off');plt.show()

# Memória vetorial
class LongTermMemoryVector:
    def __init__(self,dim=512,device=None):
        self.device=device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.dim=dim; self.keys=torch.empty((0,dim),device=self.device); self.values=[]
    def remember(self,text,value,encoder=None):
        emb=F.normalize(torch.rand(self.dim,device=self.device) if encoder is None else F.normalize(encoder(text).to(self.device)))
        self.keys=torch.cat([self.keys,emb.unsqueeze(0)],0); self.values.append(value)
    def recall(self,text,encoder=None,top_k=1):
        if len(self.values)==0: return []
        emb=F.normalize(torch.rand(self.dim,device=self.device) if encoder is None else F.normalize(encoder(text).to(self.device)))
        sim=F.cosine_similarity(emb.unsqueeze(0),self.keys); return [self.values[i] for i in sim.topk(top_k).indices.tolist()]
    def clear(self): self.keys=torch.empty((0,self.dim),device=self.device); self.values=[]

# Mini browser
class MiniBrowser:
    def __init__(self,search_engine="https://www.google.com/search?q="): self.search_engine=search_engine
    def search(self,q,num_results=3):
        try: r=requests.get(self.search_engine+q.replace(" ","+"),headers={"User-Agent":"Mozilla/5.0"})
        except Exception as e: return [str(e)]
        soup=BeautifulSoup(r.text,"html.parser"); res=[]
        for g in soup.find_all('a'):
            h=g.get('href')
            if h and h.startswith("/url?q="): res.append(h.split("/url?q=")[1].split("&")[0])
            if len(res)>=num_results: break
        return res

# Health
class HealthModule:
    def __init__(self): self.status={"memory":0,"gpu":0,"last_error":None}
    def update(self): self.status["memory"]=torch.cuda.memory_allocated() if torch.cuda.is_available() else 0; self.status["gpu"]=torch.cuda.memory_reserved() if torch.cuda.is_available() else 0
    def report_error(self,e): self.status["last_error"]=str(e)
    def get_status(self): self.update(); return self.status

# Dataset
class GPTMiniDataset(Dataset):
    def __init__(self,tokenizer,max_len=512):
        self.tokenizer=tokenizer; self.max_len=max_len
        langs=["en","pt","fr","es","de"]; self.text_dataset=concatenate_datasets([load_dataset("wikipedia",f"20220301.{l}",split="train") for l in langs])
        codes=["python","javascript","java","cpp","c"]; self.code_dataset=concatenate_datasets([load_dataset("bigcode/the-stack-dedup",split="train") for l in codes])
        self.image_dataset=load_dataset("coco","2017",split="train"); self.transform_image=T.Compose([T.Resize((256,256)),T.ToTensor()])
        try: self.reasoning_dataset=concatenate_datasets([load_dataset("gsm8k",split="train"),load_dataset("bigbench","logicaldeduction",split="train")])
        except: self.reasoning_dataset=self.text_dataset
        self.num_audio=10000
    def __len__(self): return max(len(self.text_dataset),len(self.code_dataset),len(self.image_dataset),len(self.reasoning_dataset),self.num_audio)
    def __getitem__(self,idx):
        t=self.text_dataset[idx%len(self.text_dataset)]["text"]; t_t=torch.tensor(self.tokenizer.encode(t)[:self.max_len],dtype=torch.long)
        c=self.code_dataset[idx%len(self.code_dataset)]["code"]; c_t=torch.tensor(self.tokenizer.encode(c)[:self.max_len],dtype=torch.long)
        i=self.image_dataset[idx%len(self.image_dataset)]["image"]; i_t=Image.fromarray(i) if isinstance(i,np.ndarray) else i; i_t=self.transform_image(i_t)
        r=self.reasoning_dataset[idx%len(self.reasoning_dataset)]; r_t=torch.tensor(self.tokenizer.encode(r.get("question",r.get("text","")))[:self.max_len],dtype=torch.long)
        a=torch.rand((256,1024)); v=torch.rand((16,3,256,256))
        return t_t,c_t,i_t,r_t,a,v

# GPT-Mini 6
class GPTMini(nn.Module):
    def __init__(self,vocab_size=2_000_000,embed_dim=4096,depth=24,heads=32):
        super().__init__()
        self.token_embedding=nn.Embedding(vocab_size,embed_dim); self.pos_embedding=nn.Parameter(torch.randn(1,512,embed_dim))
        self.blocks=nn.ModuleList([AdvancedTransformerBlock(embed_dim,heads,embed_dim*4) for _ in range(depth)])
        self.ln_f=nn.LayerNorm(embed_dim)
        self.text_head=nn.Linear(embed_dim,vocab_size); self.code_head=nn.Linear(embed_dim,vocab_size); self.reasoning_head=nn.Linear(embed_dim,vocab_size)
        self.image_vae=ImageVAE(dim=embed_dim//2); self.audio_fc=nn.Linear(1024,embed_dim); self.video_fc=nn.Linear(16*3*256*256,embed_dim)
        self.memory=LongTermMemoryVector(); self.browser=MiniBrowser(); self.health=HealthModule()
    def forward_text(self,x): x=self.token_embedding(x)+self.pos_embedding[:,:x.size(1)]; [x:=b(x) for b in self.blocks]; return self.text_head(self.ln_f(x))
    def forward_code(self,x): x=self.token_embedding(x)+self.pos_embedding[:,:x.size(1)]; [x:=b(x) for b in self.blocks]; return self.code_head(self.ln_f(x))
    def forward_reasoning(self,x): x=self.token_embedding(x)+self.pos_embedding[:,:x.size(1)]; [x:=b(x) for b in self.blocks]; return self.reasoning_head(self.ln_f(x))
    def forward_image(self,x): z=self.image_vae.encode(x.unsqueeze(0)); return self.image_vae.decode(z)[0]
    def forward_audio(self,x): return self.audio_fc(x.mean(0,keepdim=True))
    def forward_video(self,x): return self.video_fc(x.flatten(start_dim=1))

# Executor
class GPTMiniExecutor:
    def run_python(self,s): l={}; exec(s,{"__builtins__":{}},l); return l
    def run_terminal(self,c): r=subprocess.run(c,shell=True,capture_output=True,text=True,timeout=5); return r.stdout+r.stderr

# Geração texto
def generate_text(model,tokenizer,prompt,maxlen=200):
    model.eval(); ids=torch.tensor(tokenizer.encode(prompt),dtype=torch.long).unsqueeze(0)
    for _ in range(maxlen):
        logits=model.forward_text(ids); next_token=torch.argmax(logits[:,-1,:],dim=-1); ids=torch.cat([ids,next_token.unsqueeze(0)],1)
    return tokenizer.decode(ids[0].tolist())
